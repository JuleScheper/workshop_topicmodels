@Article{maierApplyingLDATopic2018,
  title = {Applying {{LDA}} Topic Modeling in Communication Research: {{Toward}} a Valid and Reliable Methodology},
  author = {Daniel Maier and A. Waldherr and P. Miltner and G. Wiedemann and A. Niekler and A. Keinert and B. Pfetsch and G. Heyer and U. Reber and T. H{\"a}ussler and H. Schmid-Petri and S. Adam},
  year = {2018},
  month = {apr},
  volume = {12},
  pages = {93--118},
  issn = {1931-2458},
  doi = {10.1080/19312458.2018.1430754},
  abstract = {ABSTRACTLatent Dirichlet allocation (LDA) topic models are increasingly being used in communication research. Yet, questions regarding reliability and validity of the approach have received little attention thus far. In applying LDA to textual data, researchers need to tackle at least four major challenges that affect these criteria: (a) appropriate pre-processing of the text collection; (b) adequate selection of model parameters, including the number of topics to be generated; (c) evaluation of the model?s reliability; and (d) the process of validly interpreting the resulting topics. We review the research literature dealing with these questions and propose a methodology that approaches these challenges. Our overall goal is to make LDA topic modeling more accessible to communication researchers and to ensure compliance with disciplinary standards. Consequently, we develop a brief hands-on user guide for applying LDA topic modeling. We demonstrate the value of our approach with empirical data from an ongoing research project.},
  file = {/Users/markobachl/Zotero/storage/6L5CH67P/maier18.pdf},
  journal = {Communication Methods and Measures},
  number = {2-3},
}
@Article{robertsStmPackageStructural2019,
  title = {Stm: {{An R Package}} for {{Structural Topic Models}}},
  shorttitle = {Stm},
  author = {Margaret E. Roberts and Brandon M. Stewart and Dustin Tingley},
  year = {2019},
  month = {oct},
  volume = {91},
  pages = {1--40},
  issn = {1548-7660},
  doi = {10.18637/jss.v091.i02},
  copyright = {Copyright (c) 2019 Margaret E. Roberts, Brandon M. Stewart, Dustin Tingley},
  file = {/Users/markobachl/Zotero/storage/NL8AG7H2/Roberts et al. - 2019 - stm An R Package for Structural Topic Models.pdf},
  journal = {Journal of Statistical Software},
  keywords = {LDA,R,stm,structural topic model,text analysis},
  language = {en},
  number = {1},
}
@Article{dennyTextPreprocessingUnsupervised2018,
  title = {Text {{Preprocessing For Unsupervised Learning}}: {{Why It Matters}}, {{When It Misleads}}, {{And What To Do About It}}},
  shorttitle = {Text {{Preprocessing For Unsupervised Learning}}},
  author = {Matthew J. Denny and Arthur Spirling},
  year = {2018},
  month = {apr},
  volume = {26},
  pages = {168--189},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2017.44},
  abstract = {Despite the popularity of unsupervised techniques for political science text-as-data research, the importance and implications of preprocessing decisions in this domain have received scant systematic attention. Yet, as we show, such decisions have profound effects on the results of real models for real data. We argue that substantive theory is typically too vague to be of use for feature selection, and that the supervised literature is not necessarily a helpful source of advice. To aid researchers working in unsupervised settings, we introduce a statistical procedure and software that examines the sensitivity of findings under alternate preprocessing regimes. This approach complements a researcher's substantive understanding of a problem by providing a characterization of the variability changes in preprocessing choices may induce when analyzing a particular dataset. In making scholars aware of the degree to which their results are likely to be sensitive to their preprocessing decisions, it aids replication efforts.},
  file = {/Users/markobachl/Zotero/storage/N39NJY3F/Denny und Spirling - 2018 - Text Preprocessing For Unsupervised Learning Why .pdf;/Users/markobachl/Zotero/storage/ZZY3C7WB/AA7D4DE0AA6AB208502515AE3EC6989E.html},
  journal = {Political Analysis},
  keywords = {descriptive statistics,statistical analysis of texts,unsupervised learning},
  language = {en},
  number = {2},
}
@Article{schofieldComparingApplesApple2016,
  ids = {alexandraschofieldComparingApplesApple2016},
  title = {Comparing Apples to Apple: {{The}} Effects of Stemmers on Topic Models},
  shorttitle = {Comparing {{Apples}} to {{Apple}}},
  author = {Alexandra Schofield and David Mimno},
  year = {2016},
  month = {dec},
  volume = {4},
  pages = {287--300},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00099},
  abstract = {Rule-based stemmers such as the Porter stemmer are frequently used to preprocess English corpora for topic modeling. In this work, we train and evaluate topic models on a variety of corpora using several different stemming algorithms. We examine several different quantitative measures of the resulting models, including likelihood, coherence, model stability, and entropy. Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability.},
  file = {/Users/markobachl/Zotero/storage/3PCHZ5W4/tacl_a_00099.pdf;/Users/markobachl/Zotero/storage/MURAUAR9/Schofield und Mimno - 2016 - Comparing Apples to Apple The Effects of Stemmers.pdf},
  journal = {Transactions of the Association for Computational Linguistics},
  language = {en},
}@Article{robertsStructuralTopicModels2014,
  title = {Structural Topic Models for Open-Ended Survey Responses},
  author = {Margaret E. Roberts and Brandon M. Stewart and Dustin Tingley and Christopher Lucas and Jetson Leder-Luis and Shana Kushner Gadarian and Bethany Albertson and David G. Rand},
  year = {2014},
  volume = {58},
  pages = {1064--1082},
  issn = {0092-5853},
  doi = {10.1111/ajps.12103},
  abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
  file = {/Users/markobachl/Zotero/storage/NTVYFI84/topicmodelsopenendedexperiments.pdf},
  journal = {American Journal of Political Science},
  number = {4},
}

@Article{robertsStructuralTopicModels2014,
  title = {Structural Topic Models for Open-Ended Survey Responses},
  author = {Margaret E. Roberts and Brandon M. Stewart and Dustin Tingley and Christopher Lucas and Jetson Leder-Luis and Shana Kushner Gadarian and Bethany Albertson and David G. Rand},
  year = {2014},
  volume = {58},
  pages = {1064--1082},
  issn = {0092-5853},
  doi = {10.1111/ajps.12103},
  abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
  file = {/Users/markobachl/Zotero/storage/8FNFHUP2/topicmodelsopenendedexperiments.pdf},
  journal = {American Journal of Political Science},
  number = {4},
}
@Article{davidmimnoOptimizingSemanticCoherence2011,
  title = {Optimizing Semantic Coherence in Topic Models},
  author = {David Mimno and Hanna M. Wallach and Edmund Talley and Miriam Leenders and Andrew McCallum},
  year = {2011},
  pages = {262--272},
  file = {/Users/markobachl/Zotero/storage/HSVJQCUZ/D11-1024.pdf},
  journal = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
}
